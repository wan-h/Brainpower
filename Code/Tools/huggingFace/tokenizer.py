"""
å®‰è£… pip install tokenizers
"""

from transformers import AutoTokenizer

# è¿™é‡Œä¼šå»hubä¸‹è½½
# tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

# æµ‹è¯•ä¸‹è½½åˆ°æœ¬åœ°(è¿™é‡Œå°†æ¨¡å‹ä¸­å…³äºTokenizerå‡ ä¸ªå¿…è¦çš„æ–‡ä»¶ä¸‹è½½ä¸‹æ¥å°±å¯ä»¥äº†)
tokenizer = AutoTokenizer.from_pretrained("c:\\Users\\willw\\Downloads")
input = "Hello, y'all! How are you ğŸ˜ ?"
print("============input============")
print(input)
print("============output============")
output = tokenizer(input)
print(output)
print("============subwords============")
print(tokenizer.convert_ids_to_tokens(output.input_ids))
print("============encode============")
print(tokenizer.encode(input))
print("============decode============")
print(tokenizer.decode(output.input_ids))
